{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Notebook content\n",
    "This notebook contains the steps and code to demonstrate the implementation of Semanting Chunking, to improve Retrieval Augumented Generation(RAG) in watsonx.ai in terms of both cost and performance. \n",
    "\n",
    "\n",
    "### About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Index knowledge base passages (once)\n",
    "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "In this notebook, we will focus on improving #2, the retrieval of relvant passages. \n",
    "Segmenting large-scale documents into coherent topic-based sections in digital content analysis is a significant challenge. Traditional methods(Recursive Character/Token/Sentence/Regex/Markdown Splitting) struggle with efficient splitting of content. Large documents, such as academic papers, lengthy reports, and detailed articles, are complex and contain multiple topics. These methods often miss subtle transitions or falsely identify them, leading to fragmented or overlapping sections.\n",
    "\n",
    "Instead, we will use a method that attempts to analyze a document and group chunks based on topic. There are a few methods, each with it's own strengths and drawbacks. \n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Document data loading](#data)\n",
    "- [Build up knowledge base](#build_base)\n",
    "- [Foundation Models on watsonx](#models)\n",
    "- [Generate a retrieval-augmented response to a question](#predict)\n",
    "- [Summary and next steps](#summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "##  Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.0.62 requires langchain-community<0.3.0,>=0.2.6, but you have langchain-community 0.0.38 which is incompatible.\n",
      "langchain-experimental 0.0.62 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.1.52 which is incompatible.\n",
      "langchain-ibm 0.1.10 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.1.52 which is incompatible.\n",
      "langchain-openai 0.1.13 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.1.52 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Successfully installed langchain-0.1.10 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: six>=1.10.0 in ./venv/lib/python3.10/site-packages (from lomond->ibm-watsonx-ai>=0.2.6) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.10 requires langchain-core<0.2,>=0.1.28, but you have langchain-core 0.2.18 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.18 which is incompatible.\n",
      "langchain-experimental 0.0.62 requires langchain-community<0.3.0,>=0.2.6, but you have langchain-community 0.0.38 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Successfully installed langchain-core-0.2.18\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: wget in ./venv/lib/python3.10/site-packages (3.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Successfully installed chromadb-0.3.26\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.10 requires langchain-core<0.2,>=0.1.28, but you have langchain-core 0.2.18 which is incompatible.\n",
      "langchain-experimental 0.0.62 requires langchain-community<0.3.0,>=0.2.6, but you have langchain-community 0.0.38 which is incompatible.\n",
      "semantic-router 0.0.37 requires pydantic<3.0.0,>=2.5.3, but you have pydantic 1.10.0 which is incompatible.\n",
      "semantic-router 0.0.37 requires tiktoken<0.7.0,>=0.6.0, but you have tiktoken 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Successfully installed pydantic-1.10.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.10/site-packages (from sqlalchemy==2.0.1) (4.12.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ibm-watsonx-ai in ./venv/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (2.32.3)\n",
      "Requirement already satisfied: urllib3 in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (2.1.0)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (2.1.4)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (2024.6.2)\n",
      "Requirement already satisfied: lomond in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (0.3.3)\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (0.9.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (23.2)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (2.13.5)\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.10/site-packages (from ibm-watsonx-ai) (6.11.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.5 in ./venv/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai) (2.13.5)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.5 in ./venv/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai) (2.13.5)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in ./venv/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in ./venv/lib/python3.10/site-packages (from ibm-cos-sdk-core==2.13.5->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in ./venv/lib/python3.10/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.10/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->ibm-watsonx-ai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->ibm-watsonx-ai) (3.7)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.10/site-packages (from importlib-metadata->ibm-watsonx-ai) (3.19.2)\n",
      "Requirement already satisfied: six>=1.10.0 in ./venv/lib/python3.10/site-packages (from lomond->ibm-watsonx-ai) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/Users/kaisercheng/Desktop/Projects/semchunk/venv/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"langchain==0.1.10\" | tail -n 1\n",
    "!pip install \"ibm-watsonx-ai>=0.2.6\" | tail -n 1\n",
    "!pip install -U langchain_ibm | tail -n 1\n",
    "!pip install wget | tail -n 1\n",
    "!pip install sentence-transformers | tail -n 1\n",
    "!pip install \"chromadb==0.3.26\" | tail -n 1\n",
    "!pip install \"pydantic==1.10.0\" | tail -n 1\n",
    "!pip install \"sqlalchemy==2.0.1\" | tail -n 1\n",
    "\n",
    "!pip3 install --quiet langchain_experimental langchain_openai\n",
    "!pip3 install ibm-watsonx-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the project id\n",
    "The API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n",
    "\n",
    "**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## Document data loading\n",
    "\n",
    "Download the file with State of the Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download(url, out=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"build_base\"></a>\n",
    "## Build up knowledge base\n",
    "\n",
    "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "In this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WatsonxEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# texts = text_splitter.split_documents(documents)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# from langchain_experimental.text_splitter import SemanticChunker\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibm_watsonx_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfoundation_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingTypes\n\u001b[0;32m---> 13\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mWatsonxEmbeddings\u001b[49m(\n\u001b[1;32m     14\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mEmbeddingTypes\u001b[38;5;241m.\u001b[39mIBM_SLATE_30M_ENG\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m     15\u001b[0m     url\u001b[38;5;241m=\u001b[39mcredentials[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m     apikey\u001b[38;5;241m=\u001b[39mcredentials[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapikey\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m     project_id\u001b[38;5;241m=\u001b[39mproject_id\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type = \"percentile\", breakpoint_threshold_amount=70)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# texts = text_splitter.split_documents(documents)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(texts[0].page_content)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(len(texts))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m     25\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     26\u001b[0m         chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     27\u001b[0m         length_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m,\n\u001b[1;32m     28\u001b[0m         add_start_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WatsonxEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id\n",
    "    )\n",
    "\n",
    "# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type = \"percentile\", breakpoint_threshold_amount=70)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "# print(texts[0].page_content)\n",
    "# print(len(texts))\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(texts[0].page_content)\n",
    "print(len(texts))\n",
    "## text split using traditional method specified in original code is much shorter than the one using semantic chunker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id\n",
    "    )\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compatibility watsonx.ai Embeddings with LangChain\n",
    "\n",
    " LangChain retrievals use `embed_documents` and `embed_query` under the hood to generate embedding vectors for uploaded documents and user query respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WatsonxEmbeddings in module langchain_ibm.embeddings:\n",
      "\n",
      "class WatsonxEmbeddings(pydantic.main.BaseModel, langchain_core.embeddings.embeddings.Embeddings)\n",
      " |  WatsonxEmbeddings(*, model_id: str = '', project_id: str = '', space_id: str = '', url: Optional[pydantic.types.SecretStr] = None, apikey: Optional[pydantic.types.SecretStr] = None, token: Optional[pydantic.types.SecretStr] = None, password: Optional[pydantic.types.SecretStr] = None, username: Optional[pydantic.types.SecretStr] = None, instance_id: Optional[pydantic.types.SecretStr] = None, version: Optional[pydantic.types.SecretStr] = None, params: Optional[dict] = None, verify: Union[str, bool, NoneType] = None, watsonx_embed: ibm_watsonx_ai.foundation_models.embeddings.embeddings.Embeddings = None, watsonx_client: ibm_watsonx_ai.client.APIClient = None) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WatsonxEmbeddings\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      langchain_core.embeddings.embeddings.Embeddings\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  embed_documents(self, texts: List[str]) -> List[List[float]]\n",
      " |      Embed search docs.\n",
      " |  \n",
      " |  embed_query(self, text: str) -> List[float]\n",
      " |      Embed query text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  validate_environment(values: Dict) -> Dict from pydantic.main.ModelMetaclass\n",
      " |      Validate that credentials and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain_ibm.embeddings.WatsonxEmbeddings.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'apikey': typing.Optional[pydantic.types.SecretStr]...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain_ibm.embeddings.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = None\n",
      " |  \n",
      " |  __fields__ = {'apikey': ModelField(name='apikey', type=Optional[Secret...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function WatsonxEmbeddings.valida...\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, model_id: str = '', project_id: s... ib...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.embeddings.embeddings.Embeddings:\n",
      " |  \n",
      " |  async aembed_documents(self, texts: List[str]) -> List[List[float]]\n",
      " |      Asynchronous Embed search docs.\n",
      " |  \n",
      " |  async aembed_query(self, text: str) -> List[float]\n",
      " |      Asynchronous Embed query text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WatsonxEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"models\"></a>\n",
    "## Foundation Models on `watsonx.ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining model\n",
    "You need to specify `model_id` that will be used for inferencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "model_id = ModelTypes.GRANITE_13B_CHAT_V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the model parameters\n",
    "We need to provide a set of model parameters that will influence the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain CustomLLM wrapper for watsonx model\n",
    "Initialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id.value,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select questions\n",
    "\n",
    "Get questions from the previously loaded test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': ' The president said, \"One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\" This statement was made in reference to Ketanji Brown Jackson, who was nominated by the president to serve on the United States Supreme Court.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    " You successfully completed this notebook!.\n",
    " \n",
    " You learned how to answer question using RAG using watsonx and LangChain.\n",
    " \n",
    "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright © 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIG-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
