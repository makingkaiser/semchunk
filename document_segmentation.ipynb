{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy sentence-transformers matplotlib scikit-learn nltk langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae01767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import List, Tuple\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import langdetect\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect the language of the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Detected language code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except:\n",
    "        return 'en'  # Default to English if detection fails\n",
    "\n",
    "def embed_sentences(sentences: List[str], language: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of sentences using Sentence-BERT.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): List of sentences to embed.\n",
    "        language (str): Language code for the input text.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of sentence embeddings.\n",
    "    \"\"\"\n",
    "    # Select appropriate model based on language\n",
    "    if language.startswith('zh'):\n",
    "        model_name = 'distiluse-base-multilingual-cased-v2'  # Better for chinese\n",
    "    else:\n",
    "        model_name = 'all-MiniLM-L6-v2' \n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(sentences, show_progress_bar=False)\n",
    "\n",
    "def compute_gap_scores(embeddings: np.ndarray, n: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute gap scores between consecutive sequences of embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Array of sentence embeddings.\n",
    "        n (int): Number of sentences to consider in each sequence.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: List of gap scores.\n",
    "    \"\"\"\n",
    "    gap_scores = []\n",
    "    for i in range(n, len(embeddings) - n):\n",
    "        s_before = embeddings[i-n:i]\n",
    "        s_after = embeddings[i:i+n]\n",
    "        sim_score = cosine_similarity(s_before.mean(axis=0).reshape(1, -1), \n",
    "                                      s_after.mean(axis=0).reshape(1, -1))[0][0]\n",
    "        gap_scores.append(1 - sim_score)  # Convert similarity to distance\n",
    "    #plot_gap_scores(gap_scores)\n",
    "    return gap_scores\n",
    "\n",
    "def plot_gap_scores(gap_scores: List[float]):\n",
    "    \"\"\"\n",
    "    Plot the gap scores using Matplotlib.\n",
    "\n",
    "    Args:\n",
    "        gap_scores (List[float]): List of gap scores.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gap_scores)\n",
    "    plt.title('Cosine Distances Between Sequential Embedding Pairs')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Cosine Distance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c605ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth_scores(scores: List[float], k: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Apply smoothing to the gap scores using a moving average.\n",
    "\n",
    "    Args:\n",
    "        scores (List[float]): List of gap scores.\n",
    "        k (int): Window size for smoothing.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: List of smoothed gap scores.\n",
    "    \"\"\"\n",
    "    smoothed = []\n",
    "    for i in range(len(scores)):\n",
    "        start = max(0, i - k // 2)\n",
    "        end = min(len(scores), i + k // 2 + 1)\n",
    "        smoothed.append(np.mean(scores[start:end]))\n",
    "    #plot_gap_scores(smoothed)\n",
    "    return smoothed\n",
    "\n",
    "def detect_boundaries(smoothed_scores: List[float], c: float) -> List[int]:\n",
    "    \"\"\"\n",
    "    Detect segment boundaries based on smoothed gap scores.\n",
    "\n",
    "    Args:\n",
    "        smoothed_scores (List[float]): List of smoothed gap scores.\n",
    "        c (float): Threshold parameter for boundary detection.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of detected boundary indices.\n",
    "    \"\"\"\n",
    "    depth_scores = []\n",
    "    for i in range(1, len(smoothed_scores) - 1):\n",
    "        depth = min(smoothed_scores[i] - smoothed_scores[i-1],\n",
    "                    smoothed_scores[i] - smoothed_scores[i+1])\n",
    "        depth_scores.append(depth)\n",
    "    \n",
    "    mean_depth = np.mean(depth_scores)\n",
    "    std_depth = np.std(depth_scores)\n",
    "    threshold = mean_depth + c * std_depth\n",
    "    \n",
    "    boundaries = [i + 1 for i, score in enumerate(depth_scores) if score > threshold]\n",
    "    return boundaries\n",
    "\n",
    "def cluster_segments(segments: List[str], embeddings: np.ndarray, n_clusters: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Cluster segments to identify repeated topics.\n",
    "\n",
    "    Args:\n",
    "        segments (List[str]): List of text segments.\n",
    "        embeddings (np.ndarray): Array of segment embeddings.\n",
    "        n_clusters (int): Number of clusters to form.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Cluster labels for each segment.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    return kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_document(document: str, n: int = 2, k: int = 5, c: float = 1.0) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Segment a document into coherent chunks using the described method.\n",
    "\n",
    "    Args:\n",
    "        document (str): The input document to be segmented.\n",
    "        n (int): Number of sentences to consider in each sequence for gap score computation.\n",
    "        k (int): Window size for smoothing.\n",
    "        c (float): Threshold parameter for boundary detection.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: List of segmented chunks from the document and their cluster labels.\n",
    "    \"\"\"\n",
    "    # Detect language\n",
    "    # language = detect_language(document)\n",
    "\n",
    "    # Tokenize sentences using NLTK\n",
    "    # sentences = sent_tokenize(document, language=language)\n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embed_sentences(sentences, language='en')\n",
    "    \n",
    "    # Compute gap scores\n",
    "    gap_scores = compute_gap_scores(embeddings, n)\n",
    "    \n",
    "    # Smooth scores\n",
    "    smoothed_scores = smooth_scores(gap_scores, k)\n",
    "    \n",
    "    # Detect boundaries\n",
    "    boundaries = detect_boundaries(smoothed_scores, c)\n",
    "    \n",
    "    # Create chunks based on detected boundaries\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for boundary in boundaries:\n",
    "        chunks.append(' '.join(sentences[start:boundary+n]))\n",
    "        start = boundary + n\n",
    "    chunks.append(' '.join(sentences[start:]))\n",
    "    \n",
    "    # Cluster segments\n",
    "    n_clusters = min(len(chunks), 5)  # Adjust the number of clusters as needed\n",
    "    chunk_embeddings = embed_sentences(chunks, language='en')\n",
    "    cluster_labels = cluster_segments(chunks, chunk_embeddings, n_clusters)\n",
    "    \n",
    "    return list(zip(chunks, cluster_labels)), smoothed_scores, boundaries\n",
    "\n",
    "def process_chunk(args):\n",
    "    \"\"\"Helper function for parallel processing.\"\"\"\n",
    "    chunk, language = args\n",
    "    return embed_sentences([chunk], language)[0]\n",
    "\n",
    "def parallel_embed_sentences(sentences: List[str], language: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of sentences using parallel processing.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): List of sentences to embed.\n",
    "        language (str): Language code for the input text.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of sentence embeddings.\n",
    "    \"\"\"\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        embeddings = list(executor.map(process_chunk, [(sent, language) for sent in sentences]))\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317619e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_segmentation(smoothed_scores: List[float], boundaries: List[int]):\n",
    "    \"\"\"\n",
    "    Visualize the segmentation with smoothed gap scores and segmentation markers.\n",
    "\n",
    "    Args:\n",
    "        smoothed_scores (List[float]): List of smoothed gap scores.\n",
    "        boundaries (List[int]): List of detected boundary indices.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(smoothed_scores, label='Smoothed gap score')\n",
    "    plt.scatter([b for b in boundaries if b < len(smoothed_scores)], \n",
    "                [smoothed_scores[b] for b in boundaries if b < len(smoothed_scores)],\n",
    "                color='red', label='Segmentation markers')\n",
    "    plt.xlabel('Smoothed gap score indices')\n",
    "    plt.ylabel('Smoothed gap score')\n",
    "    plt.title('Document Segmentation Visualization')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_segmentation_bars(smoothed_scores: List[float], boundaries: List[int]):\n",
    "    \"\"\"\n",
    "    Visualize the segmentation with smoothed gap scores, segmentation markers, and colored chunks.\n",
    "\n",
    "    Args:\n",
    "        smoothed_scores (List[float]): List of smoothed gap scores.\n",
    "        boundaries (List[int]): List of detected boundary indices.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot the smoothed scores\n",
    "    plt.plot(smoothed_scores)\n",
    "\n",
    "    # Set y-axis limit\n",
    "    y_upper_bound = max(smoothed_scores) * 1.1\n",
    "    plt.ylim(0, y_upper_bound)\n",
    "    plt.xlim(0, len(smoothed_scores))\n",
    "\n",
    "    # Calculate threshold (using mean + standard deviation instead of percentile)\n",
    "    threshold = np.mean(smoothed_scores) + np.std(smoothed_scores)\n",
    "    plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "\n",
    "    # Count chunks\n",
    "    num_chunks = len(boundaries) + 1\n",
    "    plt.text(x=(len(smoothed_scores)*0.01), y=y_upper_bound/50, s=f\"{num_chunks} Chunks\")\n",
    "\n",
    "    # Color and label chunks\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    start_index = 0\n",
    "    for i, end_index in enumerate(boundaries + [len(smoothed_scores)]):\n",
    "        plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([start_index, end_index]),\n",
    "                 y=threshold + (y_upper_bound) / 20,\n",
    "                 s=f\"Chunk #{i+1}\", horizontalalignment='center',\n",
    "                 rotation='vertical')\n",
    "        start_index = end_index\n",
    "\n",
    "    plt.title(\"Document Chunks Based On Embedding Breakpoints\")\n",
    "    plt.xlabel(\"Index of sentences in document (Sentence Position)\")\n",
    "    plt.ylabel(\"Smoothed gap score between sequential sentences\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify the segment_document function to return smoothed_scores and boundaries\n",
    "def segment_document(document: str, n: int = 2, k: int = 5, c: float = 1.0) -> Tuple[List[str], List[float], List[int]]:\n",
    "    \"\"\"\n",
    "    Segment a document into coherent chunks using the described method.\n",
    "\n",
    "    Args:\n",
    "        document (str): The input document to be segmented.\n",
    "        n (int): Number of sentences to consider in each sequence for gap score computation.\n",
    "        k (int): Window size for smoothing.\n",
    "        c (float): Threshold parameter for boundary detection.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: List of segmented chunks from the document and their cluster labels.\n",
    "    \"\"\"\n",
    "    # Detect language\n",
    "    # language = detect_language(document)\n",
    "\n",
    "    # Tokenize sentences using NLTK\n",
    "    # sentences = sent_tokenize(document, language=language)\n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embed_sentences(sentences, language='en')\n",
    "    \n",
    "    # Compute gap scores\n",
    "    gap_scores = compute_gap_scores(embeddings, n)\n",
    "    \n",
    "    # Smooth scores\n",
    "    smoothed_scores = smooth_scores(gap_scores, k)\n",
    "    \n",
    "    # Detect boundaries\n",
    "    boundaries = detect_boundaries(smoothed_scores, c)\n",
    "    \n",
    "    \n",
    "    # Create chunks based on detected boundaries\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for boundary in boundaries:\n",
    "        chunks.append(' '.join(sentences[start:boundary+n]))\n",
    "        start = boundary + n\n",
    "    chunks.append(' '.join(sentences[start:]))\n",
    "    \n",
    "    return chunks, smoothed_scores, boundaries\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    return chunks, smoothed_scores, boundaries\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('essays.txt', 'r') as file:\n",
    "        document = file.read()\n",
    "    chunks, smoothed_scores, boundaries = segment_document(document, n=1, k=1, c=1.0)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(chunk)\n",
    "        print()\n",
    "    \n",
    "    visualize_segmentation_bars(smoothed_scores, boundaries)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}